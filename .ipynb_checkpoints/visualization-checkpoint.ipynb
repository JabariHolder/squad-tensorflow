{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mpcm import MPCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer('train_epoch', 100, 'Training epoch')\n",
    "flags.DEFINE_integer('test_epoch', 1, 'Test for every n training epoch')\n",
    "flags.DEFINE_integer(\"batch_size\", 32, \"Size of batch (32)\")\n",
    "flags.DEFINE_integer(\"dim_perspective\", 20, \"Maximum number of perspective (20)\")\n",
    "flags.DEFINE_integer(\"dim_embed_word\", 300, \"Dimension of word embedding (300)\")\n",
    "flags.DEFINE_integer(\"dim_rnn_cell\", 100, \"Dimension of RNN cell (100)\")\n",
    "flags.DEFINE_integer(\"dim_hidden\", 100, \"Dimension of hidden layer\")\n",
    "flags.DEFINE_integer(\"num_paraphrase\", 1, \"Maximum number of question paraphrasing\")\n",
    "flags.DEFINE_integer(\"rnn_layer\", 1, \"Layer number of RNN \")\n",
    "flags.DEFINE_integer(\"context_maxlen\", 0, \"Predefined context max length\")\n",
    "flags.DEFINE_integer(\"validation_cnt\", 100, \"Number of model validation\")\n",
    "flags.DEFINE_float(\"rnn_dropout\", 0.5, \"Dropout of RNN cell\")\n",
    "flags.DEFINE_float(\"hidden_dropout\", 0.5, \"Dropout rate of hidden layer\")\n",
    "flags.DEFINE_float(\"embed_dropout\", 0.8, \"Dropout rate of embedding layer\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.00162, \"Initial learning rate of the optimzier\")\n",
    "flags.DEFINE_float(\"decay_rate\", 1.00, \"Decay rate of learning rate (0.99)\")\n",
    "flags.DEFINE_float(\"decay_step\", 100, \"Decay step of learning rate\")\n",
    "flags.DEFINE_float(\"max_grad_norm\", 5.0, \"Maximum gradient to clip\")\n",
    "flags.DEFINE_boolean(\"embed_trainable\", False, \"True to optimize embedded words\")\n",
    "flags.DEFINE_boolean(\"test\", False, \"True to run only iteration 5\")\n",
    "flags.DEFINE_boolean(\"debug\", False, \"True to show debug message\")\n",
    "flags.DEFINE_boolean(\"save\", False, \"True to save model after testing\")\n",
    "flags.DEFINE_boolean(\"sample_params\", False, \"True to sample parameters\")\n",
    "flags.DEFINE_string(\"model\", \"m\", \"b: basic, m: mpcm, q: ql_mpcm\")\n",
    "flags.DEFINE_string('train_path', './data/train-v1.1.json', 'Training dataset path')\n",
    "flags.DEFINE_string('dev_path', './data/dev-v1.1.json',  'Development dataset path')\n",
    "flags.DEFINE_string('pred_path', './result/dev-v1.1-pred.json', 'Prediction output path')\n",
    "flags.DEFINE_string('glove_path', \\\n",
    "        '~/common/glove/glove.6B.'+ str(tf.app.flags.FLAGS.dim_embed_word) +'d.txt', 'embed path')\n",
    "flags.DEFINE_string('validation_path', './result/validation.txt', 'Validation file path')\n",
    "flags.DEFINE_string('checkpoint_dir', './result/ckpt/', 'Checkpoint directory')\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent words among 103027\n",
      "[('of', 130668), ('in', 90564), ('and', 86029), ('to', 70981), ('what', 51635), ('is', 37323), ('was', 36275), ('as', 26334), ('for', 25093), ('by', 21712), ('that', 20885), ('with', 20399), ('on', 19459), ('did', 16969), ('are', 16715), ('from', 16562), ('which', 15477), ('were', 13661), ('who', 12626), ('or', 11336)]\n",
      "nb6 1\n",
      "digit cnt 4517\n",
      "alpha cnt 98643\n",
      "Dictionary size 103027\n",
      "[('', 0), ('0', 1), ('00', 2), ('000', 3), ('0000', 4), ('0000222556', 5), ('0001', 6), ('00018', 7), ('0002', 8), ('00028', 9), ('0003', 10), ('00042', 11), ('00043', 12), ('0005', 13), ('00054', 14), ('00071', 15), ('00079', 16), ('000â€“20', 17), ('001', 18), ('0012', 19)]\n",
      "Maxlen of C:581, Q:34, A:35\n",
      "Glove Loading...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'glove_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c47ef775efca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Preprocess dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_maxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_maxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mpretrained_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msaved_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context_maxlen'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mc_maxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context_maxlen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/SQuAD-tensorflow/dataset.py\u001b[0m in \u001b[0;36mload_glove\u001b[0;34m(dictionary, params)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Glove Loading...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dim_embed_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'glove_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;34m'Pretrained dimension does not match!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mglove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'glove_path'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pprint\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from model import Basic\n",
    "from ql_mpcm import QL_MPCM\n",
    "from time import gmtime, strftime\n",
    "from dataset import read_data, build_dict, load_glove, preprocess\n",
    "from run import train, test\n",
    "\n",
    "\n",
    "# Parse arguments and flags\n",
    "expected_version = '1.1'\n",
    "saved_params = FLAGS.__flags\n",
    "\n",
    "# Load dataset once\n",
    "train_path = saved_params['train_path']\n",
    "dev_path = saved_params['dev_path']\n",
    "train_dataset = read_data(train_path, expected_version)\n",
    "dev_dataset = read_data(dev_path, expected_version)\n",
    "\n",
    "\"\"\"\n",
    "Dataset is structured in json format:\n",
    "    articles (list)\n",
    "    - paragraphs (list)\n",
    "        - context\n",
    "        - qas (list)\n",
    "            - answers\n",
    "            - question\n",
    "            - id \n",
    "    - title\n",
    "\"\"\"\n",
    "# Preprocess dataset\n",
    "word2idx, idx2word, c_maxlen, q_maxlen = build_dict(train_dataset, saved_params)\n",
    "pretrained_glove, word2idx, idx2word = load_glove(word2idx, saved_params)\n",
    "if saved_params['context_maxlen'] > 0: \n",
    "    c_maxlen = saved_params['context_maxlen']\n",
    "\n",
    "train_dataset = preprocess(train_dataset, word2idx, c_maxlen, q_maxlen)\n",
    "dev_dataset = preprocess(dev_dataset, word2idx, c_maxlen, q_maxlen)\n",
    "saved_params['context_maxlen'] = c_maxlen\n",
    "saved_params['question_maxlen'] = q_maxlen\n",
    "saved_params['voca_size'] = len(word2idx)\n",
    "saved_params['dim_output'] = c_maxlen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = copy.deepcopy(saved_params)\n",
    "pprint.PrettyPrinter().pprint(params)\n",
    "\n",
    "# Make model and run experiment\n",
    "if params['model'] == 'm':\n",
    "    my_model = MPCM(params, initializer=[pretrained_glove, word2idx])\n",
    "elif params['model'] == 'q':\n",
    "    my_model = QL_MPCM(params, initializer=[pretrained_glove, word2idx])\n",
    "elif params['model'] == 'b':\n",
    "    my_model = Basic(params, initializer=[pretrained_glove, word2idx])\n",
    "else:\n",
    "    assert False, \"Check your version %s\" % params['model']\n",
    "\n",
    "my_model.load(params['checkpoint_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
